{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Edge-AI와 3D Depth Camera\n",
    "\n",
    "3D Depth Camera는 공간의 깊이 정보를 캡처하여 3차원 데이터로 변환하는 장치입니다. 이러한 카메라는 다양한 응용 분야에서 사용되며, 컴퓨터 비전, 로봇 공학, 증강현실(AR), 가상현실(VR), 의료 이미지 처리 등에서 중요합니다. 본 강의에서는 3D Depth Camera의 원리, 기술, 응용 사례, 그리고 DeepStream을 활용할 수 있는 방법에 대해 설명합니다.\n",
    "\n",
    "## 03-2. 3D Depth Camera\n",
    "\n",
    "### 3D Depth Camera의 정의\n",
    "3D Depth Camera는 객체나 장면의 깊이 정보를 캡처할 수 있는 장치로, 카메라가 각 픽셀의 거리 값을 캡처하여 3차원 공간에서 물체를 인식하거나 측정할 수 있도록 합니다.\n",
    "\n",
    "### 3D Depth Camera의 원리\n",
    "1. **스테레오 비전 (Stereo Vision)**:\n",
    "    * 두 개의 카메라로 물체를 촬영한 이미지를 비교하여 깊이를 계산합니다.\n",
    "    * 사람의 시각 시스템과 유사한 방식입니다.\n",
    "\n",
    "2. **구조광 (Structured Light)**:\n",
    "    * 특정 패턴의 빛을 물체에 투사하고, 왜곡된 패턴을 분석하여 깊이를 측정합니다.\n",
    "    * 대표적인 예: Microsoft Kinect\n",
    "\n",
    "3. **비행시간 (Time of Flight, ToF)**:\n",
    "    * 빛이 물체에 반사되어 돌아오는 시간을 측정하여 깊이를 계산합니다.\n",
    "    * 정확하고 빠른 깊이 측정이 가능합니다.\n",
    "\n",
    "4. **LiDAR (Light Detection and Ranging)**:\n",
    "    * 레이저를 사용해 주변 환경을 스캔하여 깊이를 측정합니다.\n",
    "    * 자율주행차에 주로 사용됩니다.\n",
    "\n",
    "### 응용 분야\n",
    "1. **산업 및 제조**:\n",
    "    * 로봇 비전: 로봇의 정확한 동작과 위치 결정을 지원.\n",
    "    * 품질 검사: 제품의 결함 및 크기 측정.\n",
    "2. **의료**:\n",
    "    * 3D 스캔: 인체를 스캔하여 맞춤형 의료 장비 제작.\n",
    "    * 수술 보조: 정확한 수술 계획을 위한 3D 모델 생성.\n",
    "3. **엔터테인먼트**:\n",
    "    * 증강현실/가상현실(AR/VR): 몰입형 환경 구축.\n",
    "    * 게임: 사용자 동작을 추적하여 인터랙션 제공.\n",
    "4. **도매(Retail)**:\n",
    "    * 고객 행동 분석: 매장 내 고객의 동선을 추적하고 행동을 분석.\n",
    "    * 재고 관리: 제품 크기 및 위치를 자동으로 스캔하여 재고를 효율적으로 관리.\n",
    "    * 무인 매장: 3D 데이터를 활용해 고객과 제품의 상호작용을 자동화."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-3. OAK-D 모듈\n",
    "- **OAK-D 모듈 소개**: OAK-D(OpenCV AI Kit Depth)는 AI 및 컴퓨터 비전 기능을 갖춘 카메라 모듈로, 실시간으로 깊이 정보를 제공하며 다양한 응용 분야에 적합합니다.\n",
    "\n",
    "- **주요 특징**:\n",
    "  1. **내장형 AI**: Myriad X VPU를 탑재하여 온보드 AI 처리 가능.\n",
    "  2. **스테레오 비전**: 두 개의 모노 카메라로 깊이 정보 계산.\n",
    "  3. **컬러 카메라**: 고해상도 컬러 카메라로 세부 정보를 캡처.\n",
    "  4. **플러그 앤 플레이**: USB를 통해 쉽게 연결 및 사용 가능.\n",
    "  5. **다양한 SDK 지원**: DepthAI 및 OpenCV와 같은 SDK 지원.\n",
    "\n",
    "- **응용 사례**:\n",
    "  - 로봇 내비게이션\n",
    "  - 객체 추적 및 감지\n",
    "  - 증강 현실 및 가상 현실 애플리케이션\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 예제 실습\n",
    "\n",
    "## Jetson Orin Nano에서 OAK-D PRO setting (Connection)\n",
    "\n",
    "### 준비물\n",
    "- Jetson Orin Nano\n",
    "- OAK-D PRO Camera\n",
    "- USB-C 케이블\n",
    "\n",
    "### 필수 라이브러리 설치\n",
    "라이브러리 설치\n",
    "```bash\n",
    "# 설치되어있는 OpenCV 삭제\n",
    "$ pip uninstall opencv-python\n",
    "\n",
    "# 저장소 추가\n",
    "$ sudo add-apt-repository universe\n",
    "$ sudo apt update\n",
    "\n",
    "# 필수 라이브러리 설치\n",
    "$ sudo apt install -y build-essential cmake git pkg-config libgtk-3-dev libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libxvidcore-dev libx264-dev libjpeg-dev libpng-dev libtiff-dev gfortran openexr libatlas-base-dev python3-dev python3-numpy libtbb2 libtbb-dev libdc1394-dev\n",
    "\n",
    "# OpenCV 설치\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "OAK-D 관련 드라이버 설치\n",
    "```bash\n",
    "$ wget -qO- https://docs.luxonis.com/install_dependencies.sh | bash\n",
    "$ pip3 install depthai --upgrade\n",
    "$ pip3 install blobconverter\n",
    "```\n",
    "* 드라이버 설치 이후 반드시 OAK-D 모듈 연결 USB를 뺐다가 다시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "\n",
    "# Pipeline 생성\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# 카메라 노드 추가\n",
    "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
    "xout_video = pipeline.create(dai.node.XLinkOut)\n",
    "xout_video.setStreamName(\"video\")\n",
    "\n",
    "cam_rgb.video.link(xout_video.input)\n",
    "\n",
    "# Pipeline 실행\n",
    "with dai.Device(pipeline) as device:\n",
    "    video_queue = device.getOutputQueue(name=\"video\", maxSize=1, blocking=False)\n",
    "\n",
    "    while True:\n",
    "        video_frame = video_queue.get()  # 프레임 가져오기\n",
    "        frame = video_frame.getCvFrame()\n",
    "        cv2.imshow(\"Video\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAK-D 모듈을 활용한 ObjectDetection\n",
    "\n",
    "### 필요 모델 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models/mobilenet-ssd_openvino_2022.1_6shave.blob...\n",
      "[==================================================]\n",
      "Done\n",
      "Blob file saved at: models/mobilenet-ssd_openvino_2022.1_6shave.blob\n"
     ]
    }
   ],
   "source": [
    "import blobconverter\n",
    "\n",
    "# MobileNet-SSD 모델 다운로드 및 Blob 변환\n",
    "blob_path = blobconverter.from_zoo(\n",
    "    name=\"mobilenet-ssd\",  # Model name from DepthAI Zoo\n",
    "    shaves=6,             # Number of shaves for Myriad X\n",
    "    output_dir=\"models\"   # Save directory for the blob file\n",
    ")\n",
    "\n",
    "print(f\"Blob file saved at: {blob_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "\n",
    "# DepthAI Pipeline 생성\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# 카메라 노드 추가 (RGB 카메라)\n",
    "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
    "cam_rgb.setPreviewSize(300, 300)  # 모델의 입력 크기 (300x300은 MobileNet-SSD 크기)\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setFps(30)\n",
    "\n",
    "# 객체 탐지 모델 노드 추가 (MobileNet-SSD)\n",
    "detection_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)\n",
    "detection_nn.setBlobPath(\"./models/mobilenet-ssd_openvino_2022.1_6shave.blob\")  # 모델 경로 지정\n",
    "detection_nn.setConfidenceThreshold(0.8)  # 탐지 최소 신뢰도 설정\n",
    "\n",
    "# RGB 카메라의 출력을 객체 탐지 네트워크로 연결\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "\n",
    "# RGB 프레임 출력 노드 추가\n",
    "xout_video = pipeline.create(dai.node.XLinkOut)\n",
    "xout_video.setStreamName(\"video\")\n",
    "cam_rgb.preview.link(xout_video.input)\n",
    "\n",
    "# 객체 탐지 결과 출력 노드 추가\n",
    "xout_detections = pipeline.create(dai.node.XLinkOut)\n",
    "xout_detections.setStreamName(\"detections\")\n",
    "detection_nn.out.link(xout_detections.input)\n",
    "\n",
    "# Device에서 Pipeline 실행\n",
    "with dai.Device(pipeline) as device:\n",
    "    # 출력 Queue 설정\n",
    "    video_queue = device.getOutputQueue(name=\"video\", maxSize=4, blocking=False)\n",
    "    detections_queue = device.getOutputQueue(name=\"detections\", maxSize=4, blocking=False)\n",
    "\n",
    "    labels = [\n",
    "        \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "        \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "        \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "        \"sofa\", \"train\", \"tvmonitor\"\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        # 비디오 프레임 가져오기\n",
    "        video_frame = video_queue.get()\n",
    "        frame = video_frame.getCvFrame()\n",
    "\n",
    "        # 객체 탐지 결과 가져오기\n",
    "        detections = detections_queue.get().detections\n",
    "\n",
    "        # 탐지 결과 프레임에 표시\n",
    "        for detection in detections:\n",
    "            # Bounding box 좌표 변환\n",
    "            x1 = int(detection.xmin * frame.shape[1])\n",
    "            y1 = int(detection.ymin * frame.shape[0])\n",
    "            x2 = int(detection.xmax * frame.shape[1])\n",
    "            y2 = int(detection.ymax * frame.shape[0])\n",
    "\n",
    "            # 라벨 이름 가져오기\n",
    "            label = labels[detection.label]\n",
    "\n",
    "            # Bounding box와 라벨 표시\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {int(detection.confidence * 100)}%\",\n",
    "                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # 결과 화면 표시\n",
    "        cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "        # 종료 조건\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 객체 탐지 + 깊이 측정 실시간 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9749/1566211732.py:19: DeprecationWarning: LEFT is deprecated, use CAM_B or address camera by name  instead.\n",
      "  mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
      "/tmp/ipykernel_9749/1566211732.py:20: DeprecationWarning: RIGHT is deprecated, use CAM_C or address camera by name  instead.\n",
      "  mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n"
     ]
    }
   ],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# DepthAI Pipeline 생성\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# RGB 카메라 노드 추가\n",
    "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
    "cam_rgb.setPreviewSize(300, 300)  # 모델 입력 크기\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setFps(30)\n",
    "\n",
    "# 깊이 노드 추가\n",
    "mono_left = pipeline.create(dai.node.MonoCamera)\n",
    "mono_right = pipeline.create(dai.node.MonoCamera)\n",
    "stereo = pipeline.create(dai.node.StereoDepth)\n",
    "\n",
    "mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "stereo.initialConfig.setConfidenceThreshold(255)\n",
    "\n",
    "mono_left.out.link(stereo.left)\n",
    "mono_right.out.link(stereo.right)\n",
    "\n",
    "# 객체 탐지 모델 노드 추가\n",
    "detection_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)\n",
    "detection_nn.setBlobPath(\"./models/mobilenet-ssd_openvino_2022.1_6shave.blob\")  # 모델 경로\n",
    "detection_nn.setConfidenceThreshold(0.9)\n",
    "\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "\n",
    "# 출력 노드 설정\n",
    "xout_rgb = pipeline.create(dai.node.XLinkOut)\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "cam_rgb.preview.link(xout_rgb.input)\n",
    "\n",
    "xout_depth = pipeline.create(dai.node.XLinkOut)\n",
    "xout_depth.setStreamName(\"depth\")\n",
    "stereo.depth.link(xout_depth.input)\n",
    "\n",
    "xout_nn = pipeline.create(dai.node.XLinkOut)\n",
    "xout_nn.setStreamName(\"detections\")\n",
    "detection_nn.out.link(xout_nn.input)\n",
    "\n",
    "# Device에서 Pipeline 실행\n",
    "with dai.Device(pipeline) as device:\n",
    "    # 출력 Queue 설정\n",
    "    rgb_queue = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    depth_queue = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
    "    detections_queue = device.getOutputQueue(name=\"detections\", maxSize=4, blocking=False)\n",
    "\n",
    "    labels = [\n",
    "        \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "        \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "        \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "        \"sofa\", \"train\", \"tvmonitor\"\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        # RGB 프레임 가져오기\n",
    "        in_rgb = rgb_queue.get()\n",
    "        frame = in_rgb.getCvFrame()\n",
    "\n",
    "        # 깊이 데이터 가져오기\n",
    "        in_depth = depth_queue.get()\n",
    "        depth_frame = in_depth.getFrame()  # 단일 채널 깊이 맵\n",
    "        depth_frame_color = cv2.normalize(depth_frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        depth_frame_color = cv2.applyColorMap(depth_frame_color.astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "        # 객체 탐지 결과 가져오기\n",
    "        in_detections = detections_queue.get()\n",
    "        detections = in_detections.detections\n",
    "\n",
    "        # 객체 탐지 결과를 RGB 프레임과 깊이 맵에 표시\n",
    "        for detection in detections:\n",
    "            # Bounding box 좌표 변환\n",
    "            x1 = int(detection.xmin * frame.shape[1])\n",
    "            y1 = int(detection.ymin * frame.shape[0])\n",
    "            x2 = int(detection.xmax * frame.shape[1])\n",
    "            y2 = int(detection.ymax * frame.shape[0])\n",
    "\n",
    "            # 라벨 이름 가져오기\n",
    "            label = labels[detection.label]\n",
    "\n",
    "            # Bounding box와 라벨 표시\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {int(detection.confidence * 100)}%\",\n",
    "                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            # 깊이 데이터에서 중심점의 거리 추출\n",
    "            x_center = int((x1 + x2) / 2)\n",
    "            y_center = int((y1 + y2) / 2)\n",
    "            depth_value = depth_frame[y_center, x_center]\n",
    "\n",
    "            # 깊이 정보를 화면에 표시\n",
    "            cv2.putText(frame, f\"Depth: {float(depth_value) / 10.0} cm\", (x1, y2 + 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        # 결과 화면 표시\n",
    "        cv2.imshow(\"RGB Frame\", frame)\n",
    "        cv2.imshow(\"Depth Map\", depth_frame_color)\n",
    "\n",
    "        # 종료 조건\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9749/1929540005.py:19: DeprecationWarning: LEFT is deprecated, use CAM_B or address camera by name  instead.\n",
      "  mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
      "/tmp/ipykernel_9749/1929540005.py:20: DeprecationWarning: RIGHT is deprecated, use CAM_C or address camera by name  instead.\n",
      "  mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n"
     ]
    }
   ],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# DepthAI Pipeline 생성\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# RGB 카메라 노드 추가\n",
    "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
    "cam_rgb.setPreviewSize(300, 300)  # 모델 입력 크기\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setFps(30)\n",
    "\n",
    "# 깊이 노드 추가\n",
    "mono_left = pipeline.create(dai.node.MonoCamera)\n",
    "mono_right = pipeline.create(dai.node.MonoCamera)\n",
    "stereo = pipeline.create(dai.node.StereoDepth)\n",
    "\n",
    "mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "stereo.initialConfig.setConfidenceThreshold(255)\n",
    "\n",
    "mono_left.out.link(stereo.left)\n",
    "mono_right.out.link(stereo.right)\n",
    "\n",
    "# 객체 탐지 모델 노드 추가\n",
    "detection_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)\n",
    "detection_nn.setBlobPath(\"./models/mobilenet-ssd_openvino_2022.1_6shave.blob\")  # 모델 경로\n",
    "detection_nn.setConfidenceThreshold(0.9)\n",
    "\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "\n",
    "# 출력 노드 설정\n",
    "xout_rgb = pipeline.create(dai.node.XLinkOut)\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "cam_rgb.preview.link(xout_rgb.input)\n",
    "\n",
    "xout_depth = pipeline.create(dai.node.XLinkOut)\n",
    "xout_depth.setStreamName(\"depth\")\n",
    "stereo.depth.link(xout_depth.input)\n",
    "\n",
    "xout_nn = pipeline.create(dai.node.XLinkOut)\n",
    "xout_nn.setStreamName(\"detections\")\n",
    "detection_nn.out.link(xout_nn.input)\n",
    "\n",
    "# Device에서 Pipeline 실행\n",
    "with dai.Device(pipeline) as device:\n",
    "    # 출력 Queue 설정\n",
    "    rgb_queue = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    depth_queue = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
    "    detections_queue = device.getOutputQueue(name=\"detections\", maxSize=4, blocking=False)\n",
    "\n",
    "    labels = [\n",
    "        \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "        \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "        \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "        \"sofa\", \"train\", \"tvmonitor\"\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        # RGB 프레임 가져오기\n",
    "        in_rgb = rgb_queue.get()\n",
    "        frame = in_rgb.getCvFrame()\n",
    "\n",
    "        # 깊이 데이터 가져오기\n",
    "        in_depth = depth_queue.get()\n",
    "        depth_frame = in_depth.getFrame()  # 단일 채널 깊이 맵\n",
    "\n",
    "        # 객체 탐지 결과 가져오기\n",
    "        in_detections = detections_queue.get()\n",
    "        detections = in_detections.detections\n",
    "\n",
    "        # 객체 탐지 결과를 RGB 프레임에 표시\n",
    "        for detection in detections:\n",
    "            # Bounding box 좌표 변환\n",
    "            x1 = int(detection.xmin * frame.shape[1])\n",
    "            y1 = int(detection.ymin * frame.shape[0])\n",
    "            x2 = int(detection.xmax * frame.shape[1])\n",
    "            y2 = int(detection.ymax * frame.shape[0])\n",
    "\n",
    "            # 라벨 이름 가져오기\n",
    "            label = labels[detection.label]\n",
    "\n",
    "            # Bounding box 중심점 계산\n",
    "            x_center = int((x1 + x2) / 2)\n",
    "            y_center = int((y1 + y2) / 2)\n",
    "\n",
    "            # 중심점의 깊이 값 가져오기\n",
    "            depth_value = depth_frame[y_center, x_center]\n",
    "\n",
    "            # Bounding box와 레이블 출력\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {int(detection.confidence * 100)}% | {depth_value} mm\",\n",
    "                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # 결과 화면 표시\n",
    "        cv2.imshow(\"Object Detection with Distance\", frame)\n",
    "\n",
    "        # 종료 조건\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
