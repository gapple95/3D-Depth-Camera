{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Depth Camera\n",
    "\n",
    "3D Depth Camera는 공간의 깊이 정보를 캡처하여 3차원 데이터로 변환하는 장치입니다. 이러한 카메라는 다양한 응용 분야에서 사용되며, 컴퓨터 비전, 로봇 공학, 증강현실(AR), 가상현실(VR), 의료 이미지 처리 등에서 중요합니다. 본 강의에서는 3D Depth Camera의 원리, 기술, 응용 사례, 그리고 DeepStream을 활용할 수 있는 방법에 대해 설명합니다.\n",
    "\n",
    "## 03-2. 3D Depth Camera\n",
    "\n",
    "### 3D Depth Camera의 정의\n",
    "3D Depth Camera는 객체나 장면의 깊이 정보를 캡처할 수 있는 장치로, 카메라가 각 픽셀의 거리 값을 캡처하여 3차원 공간에서 물체를 인식하거나 측정할 수 있도록 합니다.\n",
    "\n",
    "### 3D Depth Camera의 종류\n",
    "**1. 스테레오 비전 (Stereo Vision) 카메라**\n",
    "- **원리**: 두 개의 카메라(렌즈)를 사용하여 인간의 시각과 유사하게 시차(parallax)를 계산.\n",
    "- **장점**:\n",
    "  - 자연광에서 잘 작동.\n",
    "  - 넓은 측정 범위.\n",
    "  - 외부 조명에 의존하지 않음.\n",
    "- **단점**:\n",
    "  - 텍스처가 없는 표면에서는 정확도가 낮음.\n",
    "  - 계산 복잡성이 높음.\n",
    "- **대표 제품**:\n",
    "  - OAK-D.\n",
    "  - Intel RealSense D435.\n",
    "  - ZED Camera by StereoLabs.\n",
    "\n",
    " **2. 비행 시간 (Time-of-Flight, ToF) 카메라**\n",
    "- **원리**: 적외선 빛을 방출하고, 물체에 반사되어 돌아오는 시간을 측정하여 거리 계산.\n",
    "- **장점**:\n",
    "  - 빠른 응답 속도.\n",
    "  - 정밀한 거리 측정 가능.\n",
    "  - 실내 환경에 적합.\n",
    "- **단점**:\n",
    "  - 실외나 강한 햇빛 아래에서 정확도 저하.\n",
    "  - 더 높은 전력 소비.\n",
    "- **대표 제품**:\n",
    "  - Microsoft Azure Kinect.\n",
    "  - Basler Blaze.\n",
    "  - Panasonic ToF 카메라.\n",
    "\n",
    " **3. 구조화 광 (Structured Light) 카메라**\n",
    "- **원리**: 특정 패턴(격자, 점 패턴 등)의 빛을 투사하고, 반사된 패턴의 왜곡을 분석하여 깊이 계산.\n",
    "- **장점**:\n",
    "  - 높은 정확도.\n",
    "  - 텍스처가 없는 표면에서 유리.\n",
    "  - 정밀한 3D 스캔에 적합.\n",
    "- **단점**:\n",
    "  - 강한 조명 환경에서 성능 저하.\n",
    "  - 비교적 짧은 측정 거리.\n",
    "- **대표 제품**:\n",
    "  - Apple Face ID (TrueDepth Camera).\n",
    "  - Intel RealSense SR300.\n",
    "\n",
    " **4. 라이다 (LiDAR) 기반 카메라**\n",
    "- **원리**: 레이저 펄스를 방출하고, 물체에서 반사된 신호의 시간을 측정하여 거리 계산.\n",
    "- **장점**:\n",
    "  - 넓은 범위와 높은 정확도.\n",
    "  - 외부 환경에서도 성능 유지.\n",
    "  - 3D 지도 생성에 적합.\n",
    "- **단점**:\n",
    "  - 상대적으로 비싸고 전력 소모가 큼.\n",
    "- **대표 제품**:\n",
    "  - Velodyne LiDAR.\n",
    "  - iPhone Pro 시리즈 (LiDAR 센서 포함).\n",
    "\n",
    " **5. 액티브 스테레오 비전 (Active Stereo Vision) 카메라**\n",
    "- **원리**: 스테레오 비전에 적외선 패턴을 추가하여, 텍스처가 없는 표면에서도 신뢰할 수 있는 시차 계산.\n",
    "- **장점**:\n",
    "  - 텍스처 없는 물체에서도 작동.\n",
    "  - 비교적 저렴.\n",
    "- **단점**:\n",
    "  - 외부 조명에 민감.\n",
    "- **대표 제품**:\n",
    "  - Intel RealSense D455.\n",
    "  - Orbbec Astra.\n",
    "\n",
    " **6. 다중 카메라 어레이 (Multi-Camera Array)**\n",
    "- **원리**: 여러 카메라로 동일한 장면을 촬영하여 깊이 계산.\n",
    "- **장점**:\n",
    "  - 넓은 시야각.\n",
    "  - 높은 정확도.\n",
    "- **단점**:\n",
    "  - 시스템 복잡성이 큼.\n",
    "  - 비용 상승.\n",
    "- **대표 제품**:\n",
    "  - Light L16 카메라.\n",
    "\n",
    " **7. 혼합 방식 (Hybrid Technology)**\n",
    "- **원리**: 두 가지 이상의 기술(예: ToF와 구조화 광)을 결합하여 깊이 계산.\n",
    "- **장점**:\n",
    "  - 단일 기술의 한계를 극복.\n",
    "  - 다양한 환경에 적응 가능.\n",
    "- **단점**:\n",
    "  - 높은 설계 복잡도.\n",
    "- **대표 제품**:\n",
    "  - Microsoft HoloLens.\n",
    "  - Oculus Rift Depth Sensor.\n",
    "\n",
    "### **비교 표**\n",
    "\n",
    "| **카메라 종류**       | **작동 원리**              | **장점**                        | **단점**                     | **대표 제품**                  |\n",
    "|-----------------------|---------------------------|--------------------------------|-----------------------------|--------------------------------|\n",
    "| **스테레오 비전**      | 시차 계산                 | 자연광에 적합, 넓은 범위         | 텍스처 없는 표면에서 부정확  | Intel RealSense D435          |\n",
    "| **ToF**              | 빛의 왕복 시간 측정        | 빠르고 정확, 실내 적합           | 햇빛에 민감, 높은 전력 소비  | Azure Kinect, Basler Blaze    |\n",
    "| **구조화 광**          | 투사 패턴 분석            | 높은 정확도, 정밀 스캔           | 강한 조명에서 성능 저하      | Apple Face ID, RealSense SR300 |\n",
    "| **LiDAR**            | 레이저 거리 측정           | 넓은 범위, 외부 적합             | 비쌈, 전력 소모 많음         | Velodyne LiDAR, iPhone Pro    |\n",
    "| **액티브 스테레오**    | 시차 + 적외선 패턴         | 텍스처 없는 표면에 유리          | 외부 조명 민감              | RealSense D455, Orbbec Astra  |\n",
    "| **다중 카메라 어레이** | 여러 카메라로 심도 계산    | 넓은 시야, 높은 정확도           | 비용 상승                   | Light L16                    |\n",
    "| **혼합 방식**         | 여러 기술 혼합            | 기술 한계 극복                  | 설계 복잡도                 | HoloLens, Oculus Rift         |\n",
    "\n",
    "---\n",
    "\n",
    "### 응용 분야\n",
    "1. **산업 및 제조**:\n",
    "    * 로봇 비전: 로봇의 정확한 동작과 위치 결정을 지원.\n",
    "    * 품질 검사: 제품의 결함 및 크기 측정.\n",
    "2. **의료**:\n",
    "    * 3D 스캔: 인체를 스캔하여 맞춤형 의료 장비 제작.\n",
    "    * 수술 보조: 정확한 수술 계획을 위한 3D 모델 생성.\n",
    "3. **엔터테인먼트**:\n",
    "    * 증강현실/가상현실(AR/VR): 몰입형 환경 구축.\n",
    "    * 게임: 사용자 동작을 추적하여 인터랙션 제공.\n",
    "4. **도매(Retail)**:\n",
    "    * 고객 행동 분석: 매장 내 고객의 동선을 추적하고 행동을 분석.\n",
    "    * 재고 관리: 제품 크기 및 위치를 자동으로 스캔하여 재고를 효율적으로 관리.\n",
    "    * 무인 매장: 3D 데이터를 활용해 고객과 제품의 상호작용을 자동화."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-3. OAK-D 모듈\n",
    "- **OAK-D 모듈**: OAK-D(OpenCV AI Kit Depth)는 AI 및 컴퓨터 비전 기능을 갖춘 카메라 모듈로, 실시간으로 깊이 정보를 제공하며 다양한 응용 분야에 적합합니다.\n",
    "\n",
    "- **주요 특징**:\n",
    "  1. **내장형 AI**: Myriad X VPU를 탑재하여 온보드 AI 처리 가능.\n",
    "  2. **스테레오 비전**: 두 개의 모노 카메라로 깊이 정보 계산.\n",
    "  3. **컬러 카메라**: 고해상도 컬러 카메라로 세부 정보를 캡처.\n",
    "  4. **플러그 앤 플레이**: USB를 통해 쉽게 연결 및 사용 가능.\n",
    "  5. **다양한 SDK 지원**: DepthAI 및 OpenCV와 같은 SDK 지원.\n",
    "\n",
    "- **응용 사례**:\n",
    "  - 로봇 내비게이션\n",
    "  - 객체 추적 및 감지\n",
    "  - 증강 현실 및 가상 현실 애플리케이션\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 예제 실습\n",
    "\n",
    "## Jetson Orin Nano에서 OAK-D PRO setting (Connection)\n",
    "\n",
    "### 준비물\n",
    "- Jetson Orin Nano\n",
    "- OAK-D PRO Camera\n",
    "- USB-C 케이블\n",
    "\n",
    "### 필수 라이브러리 설치\n",
    "라이브러리 설치\n",
    "```bash\n",
    "# 설치되어있는 OpenCV 삭제\n",
    "$ pip uninstall opencv-python\n",
    "\n",
    "# 저장소 추가\n",
    "$ sudo add-apt-repository universe\n",
    "$ sudo apt update\n",
    "\n",
    "# 필수 라이브러리 설치\n",
    "$ sudo apt install -y build-essential cmake git pkg-config libgtk-3-dev libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libxvidcore-dev libx264-dev libjpeg-dev libpng-dev libtiff-dev gfortran openexr libatlas-base-dev python3-dev python3-numpy libtbb2 libtbb-dev libdc1394-dev\n",
    "\n",
    "# OpenCV 설치\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "OAK-D 관련 드라이버 설치\n",
    "```bash\n",
    "$ wget -qO- https://docs.luxonis.com/install_dependencies.sh | bash\n",
    "$ pip3 install depthai --upgrade\n",
    "$ pip3 install blobconverter\n",
    "```\n",
    "* 드라이버 설치 이후 반드시 OAK-D 모듈 연결 USB를 뺐다가 다시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "\n",
    "# Pipeline 생성\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# 카메라 노드 추가\n",
    "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
    "xout_video = pipeline.create(dai.node.XLinkOut)\n",
    "xout_video.setStreamName(\"video\")\n",
    "\n",
    "cam_rgb.video.link(xout_video.input)\n",
    "\n",
    "# Pipeline 실행\n",
    "with dai.Device(pipeline) as device:\n",
    "    video_queue = device.getOutputQueue(name=\"video\", maxSize=1, blocking=False)\n",
    "\n",
    "    while True:\n",
    "        video_frame = video_queue.get()  # 프레임 가져오기\n",
    "        frame = video_frame.getCvFrame()\n",
    "        cv2.imshow(\"Video\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAK-D 모듈을 활용한 객체 탐\n",
    "\n",
    "### 필요 모델 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models/mobilenet-ssd_openvino_2022.1_6shave.blob...\n",
      "[==================================================]\n",
      "Done\n",
      "Blob file saved at: models/mobilenet-ssd_openvino_2022.1_6shave.blob\n"
     ]
    }
   ],
   "source": [
    "import blobconverter\n",
    "\n",
    "# MobileNet-SSD 모델 다운로드 및 Blob 변환\n",
    "blob_path = blobconverter.from_zoo(\n",
    "    name=\"mobilenet-ssd\",  # Model name from DepthAI Zoo\n",
    "    shaves=6,             # Number of shaves for Myriad X\n",
    "    output_dir=\"models\"   # Save directory for the blob file\n",
    ")\n",
    "\n",
    "print(f\"Blob file saved at: {blob_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 객체 탐지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "\n",
    "# DepthAI Pipeline 생성\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# 카메라 노드 추가 (RGB 카메라)\n",
    "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
    "cam_rgb.setPreviewSize(300, 300)  # 모델의 입력 크기 (300x300은 MobileNet-SSD 크기)\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setFps(30)\n",
    "\n",
    "# 객체 탐지 모델 노드 추가 (MobileNet-SSD)\n",
    "detection_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)\n",
    "detection_nn.setBlobPath(\"./models/mobilenet-ssd_openvino_2022.1_6shave.blob\")  # 모델 경로 지정\n",
    "detection_nn.setConfidenceThreshold(0.8)  # 탐지 최소 신뢰도 설정\n",
    "\n",
    "# RGB 카메라의 출력을 객체 탐지 네트워크로 연결\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "\n",
    "# RGB 프레임 출력 노드 추가\n",
    "xout_video = pipeline.create(dai.node.XLinkOut)\n",
    "xout_video.setStreamName(\"video\")\n",
    "cam_rgb.preview.link(xout_video.input)\n",
    "\n",
    "# 객체 탐지 결과 출력 노드 추가\n",
    "xout_detections = pipeline.create(dai.node.XLinkOut)\n",
    "xout_detections.setStreamName(\"detections\")\n",
    "detection_nn.out.link(xout_detections.input)\n",
    "\n",
    "# Device에서 Pipeline 실행\n",
    "with dai.Device(pipeline) as device:\n",
    "    # 출력 Queue 설정\n",
    "    video_queue = device.getOutputQueue(name=\"video\", maxSize=4, blocking=False)\n",
    "    detections_queue = device.getOutputQueue(name=\"detections\", maxSize=4, blocking=False)\n",
    "\n",
    "    labels = [\n",
    "        \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "        \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "        \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "        \"sofa\", \"train\", \"tvmonitor\"\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        # 비디오 프레임 가져오기\n",
    "        video_frame = video_queue.get()\n",
    "        frame = video_frame.getCvFrame()\n",
    "\n",
    "        # 객체 탐지 결과 가져오기\n",
    "        detections = detections_queue.get().detections\n",
    "\n",
    "        # 탐지 결과 프레임에 표시\n",
    "        for detection in detections:\n",
    "            # Bounding box 좌표 변환\n",
    "            x1 = int(detection.xmin * frame.shape[1])\n",
    "            y1 = int(detection.ymin * frame.shape[0])\n",
    "            x2 = int(detection.xmax * frame.shape[1])\n",
    "            y2 = int(detection.ymax * frame.shape[0])\n",
    "\n",
    "            # 라벨 이름 가져오기\n",
    "            label = labels[detection.label]\n",
    "\n",
    "            # Bounding box와 라벨 표시\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {int(detection.confidence * 100)}%\",\n",
    "                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # 결과 화면 표시\n",
    "        cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "        # 종료 조건\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAK-D 모듈을 활용한 객체 탐지 및 깊이 측정\n",
    "\n",
    "### 객체 탐 및 깊이 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9749/2756853184.py:19: DeprecationWarning: LEFT is deprecated, use CAM_B or address camera by name  instead.\n",
      "  mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
      "/tmp/ipykernel_9749/2756853184.py:20: DeprecationWarning: RIGHT is deprecated, use CAM_C or address camera by name  instead.\n",
      "  mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n"
     ]
    }
   ],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# DepthAI Pipeline 생성\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# RGB 카메라 노드 추가\n",
    "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
    "cam_rgb.setPreviewSize(300, 300)  # 모델 입력 크기\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setFps(30)\n",
    "\n",
    "# 깊이 노드 추가\n",
    "mono_left = pipeline.create(dai.node.MonoCamera)\n",
    "mono_right = pipeline.create(dai.node.MonoCamera)\n",
    "stereo = pipeline.create(dai.node.StereoDepth)\n",
    "\n",
    "mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "stereo.initialConfig.setConfidenceThreshold(255)\n",
    "\n",
    "mono_left.out.link(stereo.left)\n",
    "mono_right.out.link(stereo.right)\n",
    "\n",
    "# 객체 탐지 모델 노드 추가\n",
    "detection_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)\n",
    "detection_nn.setBlobPath(\"./models/mobilenet-ssd_openvino_2022.1_6shave.blob\")  # 모델 경로\n",
    "detection_nn.setConfidenceThreshold(0.9)\n",
    "\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "\n",
    "# 출력 노드 설정\n",
    "xout_rgb = pipeline.create(dai.node.XLinkOut)\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "cam_rgb.preview.link(xout_rgb.input)\n",
    "\n",
    "xout_depth = pipeline.create(dai.node.XLinkOut)\n",
    "xout_depth.setStreamName(\"depth\")\n",
    "stereo.depth.link(xout_depth.input)\n",
    "\n",
    "xout_nn = pipeline.create(dai.node.XLinkOut)\n",
    "xout_nn.setStreamName(\"detections\")\n",
    "detection_nn.out.link(xout_nn.input)\n",
    "\n",
    "# Device에서 Pipeline 실행\n",
    "with dai.Device(pipeline) as device:\n",
    "    # 출력 Queue 설정\n",
    "    rgb_queue = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    depth_queue = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
    "    detections_queue = device.getOutputQueue(name=\"detections\", maxSize=4, blocking=False)\n",
    "\n",
    "    labels = [\n",
    "        \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "        \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "        \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "        \"sofa\", \"train\", \"tvmonitor\"\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        # RGB 프레임 가져오기\n",
    "        in_rgb = rgb_queue.get()\n",
    "        frame = in_rgb.getCvFrame()\n",
    "\n",
    "        # 깊이 데이터 가져오기\n",
    "        in_depth = depth_queue.get()\n",
    "        depth_frame = in_depth.getFrame()  # 단일 채널 깊이 맵\n",
    "\n",
    "        # 객체 탐지 결과 가져오기\n",
    "        in_detections = detections_queue.get()\n",
    "        detections = in_detections.detections\n",
    "\n",
    "        # 객체 탐지 결과를 RGB 프레임에 표시\n",
    "        for detection in detections:\n",
    "            # Bounding box 좌표 변환\n",
    "            x1 = int(detection.xmin * frame.shape[1])\n",
    "            y1 = int(detection.ymin * frame.shape[0])\n",
    "            x2 = int(detection.xmax * frame.shape[1])\n",
    "            y2 = int(detection.ymax * frame.shape[0])\n",
    "\n",
    "            # 라벨 이름 가져오기\n",
    "            label = labels[detection.label]\n",
    "\n",
    "            # Bounding box 중심점 계산\n",
    "            x_center = int((x1 + x2) / 2)\n",
    "            y_center = int((y1 + y2) / 2)\n",
    "\n",
    "            # 중심점의 깊이 값 가져오기 (m로 변환)\n",
    "            depth_value = depth_frame[y_center, x_center]  # 단위: mm\n",
    "\n",
    "            # Bounding box와 레이블 출력\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {int(detection.confidence * 100)}% | {depth_value} mm\",\n",
    "                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # 결과 화면 표시\n",
    "        cv2.imshow(\"Object Detection with Distance\", frame)\n",
    "\n",
    "        # 종료 조건\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 깊이 측정 실시간 시각화\n",
    "* RGB 프레임에 객체 탐지 결과를 표시.\n",
    "* 깊이 맵은 COLORMAP_JET로 컬러 변환하여 시각적으로 표현."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9749/1566211732.py:19: DeprecationWarning: LEFT is deprecated, use CAM_B or address camera by name  instead.\n",
      "  mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
      "/tmp/ipykernel_9749/1566211732.py:20: DeprecationWarning: RIGHT is deprecated, use CAM_C or address camera by name  instead.\n",
      "  mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n"
     ]
    }
   ],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# DepthAI Pipeline 생성\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# RGB 카메라 노드 추가\n",
    "cam_rgb = pipeline.create(dai.node.ColorCamera)\n",
    "cam_rgb.setPreviewSize(300, 300)  # 모델 입력 크기\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setFps(30)\n",
    "\n",
    "# 깊이 노드 추가\n",
    "mono_left = pipeline.create(dai.node.MonoCamera)\n",
    "mono_right = pipeline.create(dai.node.MonoCamera)\n",
    "stereo = pipeline.create(dai.node.StereoDepth)\n",
    "\n",
    "mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "stereo.initialConfig.setConfidenceThreshold(255)\n",
    "\n",
    "mono_left.out.link(stereo.left)\n",
    "mono_right.out.link(stereo.right)\n",
    "\n",
    "# 객체 탐지 모델 노드 추가\n",
    "detection_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)\n",
    "detection_nn.setBlobPath(\"./models/mobilenet-ssd_openvino_2022.1_6shave.blob\")  # 모델 경로\n",
    "detection_nn.setConfidenceThreshold(0.9)\n",
    "\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "\n",
    "# 출력 노드 설정\n",
    "xout_rgb = pipeline.create(dai.node.XLinkOut)\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "cam_rgb.preview.link(xout_rgb.input)\n",
    "\n",
    "xout_depth = pipeline.create(dai.node.XLinkOut)\n",
    "xout_depth.setStreamName(\"depth\")\n",
    "stereo.depth.link(xout_depth.input)\n",
    "\n",
    "xout_nn = pipeline.create(dai.node.XLinkOut)\n",
    "xout_nn.setStreamName(\"detections\")\n",
    "detection_nn.out.link(xout_nn.input)\n",
    "\n",
    "# Device에서 Pipeline 실행\n",
    "with dai.Device(pipeline) as device:\n",
    "    # 출력 Queue 설정\n",
    "    rgb_queue = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    depth_queue = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
    "    detections_queue = device.getOutputQueue(name=\"detections\", maxSize=4, blocking=False)\n",
    "\n",
    "    labels = [\n",
    "        \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "        \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "        \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "        \"sofa\", \"train\", \"tvmonitor\"\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        # RGB 프레임 가져오기\n",
    "        in_rgb = rgb_queue.get()\n",
    "        frame = in_rgb.getCvFrame()\n",
    "\n",
    "        # 깊이 데이터 가져오기\n",
    "        in_depth = depth_queue.get()\n",
    "        depth_frame = in_depth.getFrame()  # 단일 채널 깊이 맵\n",
    "        depth_frame_color = cv2.normalize(depth_frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        depth_frame_color = cv2.applyColorMap(depth_frame_color.astype(np.uint8), cv2.COLORMAP_JET)\n",
    "\n",
    "        # 객체 탐지 결과 가져오기\n",
    "        in_detections = detections_queue.get()\n",
    "        detections = in_detections.detections\n",
    "\n",
    "        # 객체 탐지 결과를 RGB 프레임과 깊이 맵에 표시\n",
    "        for detection in detections:\n",
    "            # Bounding box 좌표 변환\n",
    "            x1 = int(detection.xmin * frame.shape[1])\n",
    "            y1 = int(detection.ymin * frame.shape[0])\n",
    "            x2 = int(detection.xmax * frame.shape[1])\n",
    "            y2 = int(detection.ymax * frame.shape[0])\n",
    "\n",
    "            # 라벨 이름 가져오기\n",
    "            label = labels[detection.label]\n",
    "\n",
    "            # Bounding box와 라벨 표시\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {int(detection.confidence * 100)}%\",\n",
    "                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            # 깊이 데이터에서 중심점의 거리 추출\n",
    "            x_center = int((x1 + x2) / 2)\n",
    "            y_center = int((y1 + y2) / 2)\n",
    "            depth_value = depth_frame[y_center, x_center]\n",
    "\n",
    "            # 깊이 정보를 화면에 표시\n",
    "            cv2.putText(frame, f\"Depth: {depth_value} mm\", (x1, y2 + 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        # 결과 화면 표시\n",
    "        cv2.imshow(\"RGB Frame\", frame)\n",
    "        cv2.imshow(\"Depth Map\", depth_frame_color)\n",
    "\n",
    "        # 종료 조건\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
